    # 10.64.3.7
    # 10.64.3.8
    # 10.66.3.86

    # Kubernetes 1.6.2
    # Docker 17.04.0-ce
    # Etcd 3.1.6
    # Flanneld 0.7.1 vxlan 网络
    # TLS 认证通信 (所有组件，如 etcd、kubernetes master 和 node)
    # RBAC 授权
    # kubelet TLS BootStrapping
    # kubedns、dashboard、heapster (influxdb、grafana)、EFK (elasticsearch、fluentd、kibana) 插件
    # 私有 docker registry，使用 ceph rgw 后端存储，TLS + HTTP Basic 认证

    # 由10.64.3.7牵头
    touch environment.sh
    vi !$

    mkdir -p /root/local/bin
    cp environment.sh /root/local/bin
    # x3 

    # 创建 CA 证书和秘钥

yum -y install ntp && ntpdate -u cn.pool.ntp.org

    #   安装 CFSSL
    yum install wget -y
    wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
    chmod +x cfssl_linux-amd64
    sudo mv cfssl_linux-amd64 /root/local/bin/cfssl

    wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
    chmod +x cfssljson_linux-amd64
    sudo mv cfssljson_linux-amd64 /root/local/bin/cfssljson

    wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
    chmod +x cfssl-certinfo_linux-amd64
    sudo mv cfssl-certinfo_linux-amd64 /root/local/bin/cfssl-certinfo

cd /root
    export PATH=/root/local/bin:$PATH
    mkdir ssl
    cd ssl
# 这个应该长久变量吧?

# 生成模板
# 生成证书信息文件
    cfssl print-defaults config > config.json
    cfssl print-defaults csr > csr.json
# CloudFlare 公司的一个 PKI 工具,json 生成 证书

# https://www.cnblogs.com/fanqisoft/p/10765038.html
# 待验证

    #   创建 CA
    # 创建 CA 配置文件
    ca-config.json

    # 创建 CA 证书签名请求
    ca-csr.json

    # 生成 CA 证书和私钥,唯一
cfssl gencert -initca ca-csr.json | cfssljson -bare ca

    #   分发证书
    sudo mkdir -p /etc/kubernetes/ssl

    sudo cp ca* /etc/kubernetes/ssl
    scp ca* root@10.66.3.86:/etc/kubernetes/ssl
    scp ca* root@10.64.3.8:/etc/kubernetes/ssl

    ll /etc/kubernetes/ssl

#   校验证书
    # 目前应该尚未生成,回头再验证

    #     使用 openssl 命令
    cd /etc/kubernetes/ssl/
    openssl x509 -noout -text -in kubernetes.pem
    # 过期 kubernetes.pem

    #     使用 cfssl-certinfo 命令
    cfssl-certinfo -cert kubernetes.pem

# 确认 Issuer 字段的内容和 ca-csr.json 一致；
# 确认 Subject 字段的内容和 kubernetes-csr.json 一致；
# 确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；
# 确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetes profile 一致



  # 部署高可用 Etcd 集群
  # etcd-host0：10.64.3.7
  # etcd-host1：10.64.3.8
  # etcd-host2：10.66.3.86

# 需要部署3次,部分为零时变量
#   使用的变量
    export NODE_NAME=etcd-host0 
    export NODE_IP=10.64.3.7
    export NODE_IPS="10.64.3.7 10.64.3.8 10.66.3.86" 
    export ETCD_NODES=etcd-host0=https://10.64.3.7:2380,etcd-host1=https://10.64.3.8:2380,etcd-host2=https://10.66.3.86:2380
    source /root/local/bin/environment.sh



    export NODE_NAME=etcd-host1 
    export NODE_IP=10.64.3.8
    export NODE_IPS="10.64.3.7 10.64.3.8 10.66.3.86" 
    export ETCD_NODES=etcd-host0=https://10.64.3.7:2380,etcd-host1=https://10.64.3.8:2380,etcd-host2=https://10.66.3.86:2380
    source /root/local/bin/environment.sh



    export NODE_NAME=etcd-host2 
    export NODE_IP=10.66.3.86
    export NODE_IPS="10.64.3.7 10.64.3.8 10.66.3.86" 
    export ETCD_NODES=etcd-host0=https://10.64.3.7:2380,etcd-host1=https://10.64.3.8:2380,etcd-host2=https://10.66.3.86:2380
    source /root/local/bin/environment.sh



#   下载二进制文件
    yum install -y wget
    wget https://github.com/coreos/etcd/releases/download/v3.1.6/etcd-v3.1.6-linux-amd64.tar.gz

    cd /root/
    scp root@10.64.3.7:/root/etcd-v3.1.6-linux-amd64.tar.gz .

    tar -xvf etcd-v3.1.6-linux-amd64.tar.gz
    sudo mv etcd-v3.1.6-linux-amd64/etcd* /root/local/bin

    #   创建 TLS 秘钥和证书
    # 创建 etcd 证书签名请求

    cd /root
    mkdir ssl
    cd ~/ssl
    export PATH=/root/local/bin:$PATH

    etcd-csr.json 

    # 生成 etcd 证书和私钥
    cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
    -ca-key=/etc/kubernetes/ssl/ca-key.pem \
    -config=/etc/kubernetes/ssl/ca-config.json \
    -profile=kubernetes etcd-csr.json | cfssljson -bare etcd

    ls etcd*

    # 这里生成了3套证书
    # kubernetes 和 etcd 证书中包含有使用该证书的 IP，所以需要对各节点分别生成，不能共享

    sudo mkdir -p /etc/etcd/ssl
    sudo mv etcd*.pem /etc/etcd/ssl
    rm etcd.csr  etcd-csr.json

    #   创建 etcd 的 systemd unit 文件
    sudo mkdir -p /var/lib/etcd 
    etcd.service

    #   启动 etcd 服务
    firewall-cmd --zone=public --add-port=2379/tcp --permanent
    firewall-cmd --zone=public --add-port=2380/tcp --permanent
    firewall-cmd --reload

    # 注意这里要线性操作,一台好了再做一台

    sudo mv etcd.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable etcd

    sudo systemctl start etcd
    systemctl status etcd

    #   验证服务
    for ip in ${NODE_IPS}; do
      ETCDCTL_API=3 /root/local/bin/etcdctl \
      --endpoints=https://${ip}:2379  \
      --cacert=/etc/kubernetes/ssl/ca.pem \
      --cert=/etc/etcd/ssl/etcd.pem \
      --key=/etc/etcd/ssl/etcd-key.pem \
      endpoint health; done




# 下载和配置 Kubectl 命令行工具
    # kubectl 默认从 ~/.kube/config 配置文件获取访问 kube-apiserver 地址、证书、用户名等信息

    # 需要将下载的 kubectl 二进制程序和生成的 ~/.kube/config 配置文件拷贝到所有使用 kubectl 命令的机器
    # 这样看来,配置应该是拷贝的,不知道证书怎么做,只要一个,还是分别生成?防火墙

    #   使用的变量
    export MASTER_IP=10.64.3.7
    export KUBE_APISERVER="https://${MASTER_IP}:6443"

    #   下载 kubectl x3
    # wget https://dl.k8s.io/v1.6.2/kubernetes-client-linux-amd64.tar.gz

    cd /root
    scp root@10.64.3.7:/root/kubernetes-client-linux-amd64.tar.gz .

    tar -xzvf kubernetes-client-linux-amd64.tar.gz
    sudo cp kubernetes/client/bin/kube* /root/local/bin/
    chmod a+x /root/local/bin/kube*
    export PATH=/root/local/bin:$PATH

    #   创建 admin 证书
    cd ssl
    admin-csr.json

    cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
      -ca-key=/etc/kubernetes/ssl/ca-key.pem \
      -config=/etc/kubernetes/ssl/ca-config.json \
      -profile=kubernetes admin-csr.json | cfssljson -bare admin

    ls admin*
    sudo mv admin*.pem /etc/kubernetes/ssl/
    rm admin.csr admin-csr.json

    #   创建 kubectl kubeconfig 文件
    # 设置集群参数
    kubectl config set-cluster kubernetes \
    --certificate-authority=/etc/kubernetes/ssl/ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER}

    # 设置客户端认证参数
    kubectl config set-credentials admin \
    --client-certificate=/etc/kubernetes/ssl/admin.pem \
    --embed-certs=true \
    --client-key=/etc/kubernetes/ssl/admin-key.pem

    # 设置上下文参数
    kubectl config set-context kubernetes \
    --cluster=kubernetes \
    --user=admin

    # 设置默认上下文
    kubectl config use-context kubernetes

    #   分发 kubeconfig 文件
    # 将 ~/.kube/config 文件拷贝到运行 kubelet 命令的机器的 ~/.kube/ 目录下

    # 三段配置+激活,生成文件写有密钥

    mkdir .kube
    cd !$
    scp root@10.64.3.7:/root/.kube/config .

    # kubectl api-versions 目前不可用

    # 使用 kubectl  的机器都需要


    # 配置 Flannel 网络 三端都要
    # kubernetes 要求集群内各节点能通过 Pod 网段互联互通，使用 Flannel 在所有节点 (Master、Node) 上创建互联互通的 Pod 网段
    #   使用的变量 
    export NODE_IP=10.64.3.7
    source /root/local/bin/environment.sh

    # 我猜是只需要在在一台机器上操作,后续观察

    #   创建 TLS 秘钥和证书 
    # etcd 集群启用了双向 TLS 认证，所以需要为 flanneld 指定与 etcd 集群通信的 CA 和秘钥
    # 创建 flanneld 证书签名请求
    cd ssl
    flanneld-csr.json

     cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
     -ca-key=/etc/kubernetes/ssl/ca-key.pem \
     -config=/etc/kubernetes/ssl/ca-config.json \
     -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld

     ls flanneld*
     sudo mkdir -p /etc/flanneld/ssl
     sudo mv flanneld*.pem /etc/flanneld/ssl
     rm flanneld.csr  flanneld-csr.json

     # 证书里ip倒是没有,先分别建吧

    #   向 etcd 写入集群 Pod 网段信息
    # 只需在第一次部署 Flannel 网络时执行，后续在其它节点上部署 Flannel 时无需再写入该信息！

    /root/local/bin/etcdctl \
      --endpoints=${ETCD_ENDPOINTS} \
      --ca-file=/etc/kubernetes/ssl/ca.pem \
      --cert-file=/etc/flanneld/ssl/flanneld.pem \
      --key-file=/etc/flanneld/ssl/flanneld-key.pem \
      set ${FLANNEL_ETCD_PREFIX}/config '{"Network":"'${CLUSTER_CIDR}'", "SubnetLen": 24, "Backend": {"Type": "vxlan"}}'

    #   安装和配置 flanneld
    #     下载 flanneld
    cd ~
    mkdir flannel

    # wget https://github.com/coreos/flannel/releases/download/v0.7.1/flannel-v0.7.1-linux-amd64.tar.gz

    scp root@10.64.3.7:/root/flannel-v0.7.1-linux-amd64.tar.gz .

    tar -xzvf flannel-v0.7.1-linux-amd64.tar.gz -C flannel
    sudo cp flannel/{flanneld,mk-docker-opts.sh} /root/local/bin
    #     创建 flanneld 的 systemd unit 文件

    # 检查网络环境
      # mk-docker-opts.sh
      # --iface=ens33
    flanneld.service


    #     启动 flanneld
    sudo cp flanneld.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable flanneld

    # 集群的东西,建议还是一个一个启动好

    sudo systemctl start flanneld
    systemctl status flanneld

    #     检查 flanneld 服务
    journalctl  -u flanneld |grep 'Lease acquired'
    ifconfig flannel.1

    172.30.95.0/32 7
    172.30.99.0/32 8
    172.30.101.0/32 86

    #   检查分配给各 flanneld 的 Pod 网段信息
     # 查看集群 Pod 网段(/16)
     /root/local/bin/etcdctl \
     --endpoints=${ETCD_ENDPOINTS} \
     --ca-file=/etc/kubernetes/ssl/ca.pem \
     --cert-file=/etc/flanneld/ssl/flanneld.pem \
     --key-file=/etc/flanneld/ssl/flanneld-key.pem \
     get ${FLANNEL_ETCD_PREFIX}/config
     # { "Network": "172.30.0.0/16", "SubnetLen": 24, "Backend": { "Type": "vxlan" } }

     # 查看已分配的 Pod 子网段列表(/24)
     /root/local/bin/etcdctl \
     --endpoints=${ETCD_ENDPOINTS} \
     --ca-file=/etc/kubernetes/ssl/ca.pem \
     --cert-file=/etc/flanneld/ssl/flanneld.pem \
     --key-file=/etc/flanneld/ssl/flanneld-key.pem \
     ls ${FLANNEL_ETCD_PREFIX}/subnets
    # /kubernetes/network/subnets/172.30.19.0-24

     # 查看某一 Pod 网段对应的 flanneld 进程监听的 IP 和网络参数
     /root/local/bin/etcdctl \
     --endpoints=${ETCD_ENDPOINTS} \
     --ca-file=/etc/kubernetes/ssl/ca.pem \
     --cert-file=/etc/flanneld/ssl/flanneld.pem \
     --key-file=/etc/flanneld/ssl/flanneld-key.pem \
     get ${FLANNEL_ETCD_PREFIX}/subnets/172.30.18.0-24
    # {"PublicIP":"10.64.3.7","BackendType":"vxlan","BackendData":{"VtepMAC":"d6:51:2e:80:5c:69"}}

    #   确保各节点间 Pod 网段能互联互通
    /root/local/bin/etcdctl \
      --endpoints=${ETCD_ENDPOINTS} \
      --ca-file=/etc/kubernetes/ssl/ca.pem \
      --cert-file=/etc/flanneld/ssl/flanneld.pem \
      --key-file=/etc/flanneld/ssl/flanneld-key.pem \
      ls ${FLANNEL_ETCD_PREFIX}/subnets
    # /kubernetes/network/subnets/172.30.99.0-24
    # /kubernetes/network/subnets/172.30.101.0-24
    # /kubernetes/network/subnets/172.30.95.0-24


    # iptables -A FORWARD -s 172.30.0.0/16 -j ACCEPT
    # firewall-cmd --add-rich-rule="rule family='ipv4' source address='172.30.0.0/16' accept"
    # 两边都没有说明,先不开防火墙吧
    # ping 172.30.18.1
    # ping 172.30.16.2
    # ping 172.30.39.3
    # 在各节点上分配 ping 这三个网段的网关地址，确保能通
    # 不是很明白

    cat /run/flannel/docker
    # 怀疑重启etcd挂了




    # 部署 Master 节点
    #   使用的变量
    export MASTER_IP=10.64.3.7
    source /root/local/bin/environment.sh

    #   下载最新版本的二进制文件,下面一种好一点

    wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.2/kubernetes.tar.gz

    tar -xzvf kubernetes.tar.gz
    cd kubernetes
    ./cluster/get-kube-binaries.sh
    # 先布一台, 从googleapis下载

    # https://github.com/kubernetes/kubernetes/releases
    # https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md

    # # wget https://dl.k8s.io/v1.6.2/kubernetes-client-linux-amd64.tar.gz
    wget https://dl.k8s.io/v1.6.2/kubernetes-server-linux-amd64.tar.gz
    # tar -xzvf kubernetes-server-linux-amd64.tar.gz
    # cd kubernetes
    # tar -xzvf  kubernetes-src.tar.gz
    # 要解压下载下来的server包

    sudo cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /root/local/bin/

    # 没有文件

    #   安装和配置 flanneld
    #   创建 kubernetes 证书
    cd ~/ssl
    kubernetes-csr.json

    firewall-cmd --zone=public --add-port=6443/tcp --permanent
    firewall-cmd --reload

    # kubectl get svc kubernetes 后期

    cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
    -ca-key=/etc/kubernetes/ssl/ca-key.pem \
    -config=/etc/kubernetes/ssl/ca-config.json \
    -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes

    ls kubernetes*
    sudo mkdir -p /etc/kubernetes/ssl/
    sudo mv kubernetes*.pem /etc/kubernetes/ssl/
    rm kubernetes.csr  kubernetes-csr.json


    #   配置和启动 kube-apiserver

    # kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求
    # kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token.csv 一致，
    # 如果一致则自动为 kubelet生成证书和秘钥

    #     创建 kube-apiserver 使用的客户端 token 文件
    cat > token.csv <<EOF
    ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
    EOF

    mv token.csv /etc/kubernetes/
    #     创建 kube-apiserver 的 systemd unit 文件

    kube-apiserver.service
    #     启动 kube-apiserver
    sudo cp kube-apiserver.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable kube-apiserver
    sudo systemctl start kube-apiserver
    sudo systemctl status kube-apiserver


    # 如果使用了 kubelet TLS Boostrap 机制，
    # 则不能再指定 --kubelet-certificate-authority、--kubelet-client-certificate 和 --kubelet-client-key 选项，
    # 否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误；
    # 不是很明白



    #   配置和启动 kube-controller-manager
    #     创建 kube-controller-manager 的 systemd unit 文件
    kube-controller-manager.service
    # kubectl get componentstatuses

    #     启动 kube-controller-manager
    sudo cp kube-controller-manager.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable kube-controller-manager
    sudo systemctl start kube-controller-manager

    #   配置和启动 kube-scheduler
    #     创建 kube-scheduler 的 systemd unit 文件
    kube-scheduler.service

    #     启动 kube-scheduler
    sudo cp kube-scheduler.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable kube-scheduler
    sudo systemctl start kube-scheduler

    #   验证 master 节点功能
    kubectl get componentstatuses




# 部署 Node 节点
  # 感觉master有3台,节点有多个,后面验证

    #   使用的变量
    export MASTER_IP=10.64.3.7
    export KUBE_APISERVER="https://${MASTER_IP}:6443"
    export NODE_IP=10.64.3.7
    source /root/local/bin/environment.sh

# 第二台
# 替换为 kubernetes master 集群任一机器 IP
export MASTER_IP=10.64.3.7
export KUBE_APISERVER="https://${MASTER_IP}:6443"
export NODE_IP=10.64.3.8
source /root/local/bin/environment.sh

#   安装和配置 flanneld
#   安装和配置 docker
#     下载最新的 docker 二进制文件
# wget https://get.docker.com/builds/Linux/x86_64/docker-17.04.0-ce.tgz

scp root@10.64.3.7:/root/docker-17.04.0-ce.tgz .

tar -xvf docker-17.04.0-ce.tgz
cp docker/docker* /root/local/bin
cp docker/completion/bash/docker /etc/bash_completion.d/

# dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中；

# flanneld 启动时将网络配置写入到 /run/flannel/docker 文件中的变量 DOCKER_NETWORK_OPTIONS，dockerd 命令行上指定该变量值来设置 docker0 网桥参数；

# 如果指定了多个 EnvironmentFile 选项，则必须将 /run/flannel/docker 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；

# 不能关闭默认开启的 --iptables 和 --ip-masq 选项；

# 如果内核版本比较新，建议使用 overlay 存储驱动


#     创建 docker 的 systemd unit 文件
cd ~

docker.service

sudo iptables -P FORWARD ACCEPT

vi /etc/rc.local
  sleep 60 && /sbin/iptables -P FORWARD ACCEPT

# 防火墙的配置先,放一下回头补充,注意要处理3台

mkdir /etc/docker/
cd !$
cat > daemon.json <<EOF
{
  "registry-mirrors": ["https://docker.mirrors.ustc.edu.cn", "hub-mirror.c.163.com"],
  "max-concurrent-downloads": 10
}
EOF


#     启动 dockerd
cd ~
sudo cp docker.service /etc/systemd/system/docker.service
sudo systemctl daemon-reload
sudo systemctl stop firewalld
sudo systemctl disable firewalld
sudo iptables -F && sudo iptables -X && sudo iptables -F -t nat && sudo iptables -X -t nat
sudo systemctl enable docker
sudo systemctl start docker

#     检查 docker 服务
docker version

# kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，
# 需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，
# 然后 kubelet 才有权限创建认证请求(certificatesigningrequests)



# #     下载最新的 kubelet 和 kube-proxy 二进制文件 master已有
# wget https://dl.k8s.io/v1.6.2/kubernetes-server-linux-amd64.tar.gz
# tar -xzvf kubernetes-server-linux-amd64.tar.gz
# cd kubernetes
cd /root/kubernetes/server/kubernetes
# 从master上copy
# tar -xzvf  kubernetes-src.tar.gz
sudo cp -r ./server/bin/{kube-proxy,kubelet} /root/local/bin/

/root/local/bin/
scp root@10.64.3.7:/root/local/bin/{kube-proxy,kubelet} .


#     创建 kubelet bootstrapping kubeconfig 文件
# 感觉也不需要每台都设,待确认
# 应该是要的,对服务设的

# --embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；
# 设置 kubelet 客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成

cd /root

# 设置集群参数
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/ssl/ca.pem \
--embed-certs=true \
--server=${KUBE_APISERVER} \
--kubeconfig=bootstrap.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap \
--token=${BOOTSTRAP_TOKEN} \
--kubeconfig=bootstrap.kubeconfig

# 设置上下文参数
kubectl config set-context default \
--cluster=kubernetes \
--user=kubelet-bootstrap \
--kubeconfig=bootstrap.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
mv bootstrap.kubeconfig /etc/kubernetes/

# 理论上只要做一次,后面尝试,不能用再添加
# 是的,只有第一次配置

#     创建 kubelet 的 systemd unit 文件
# --address 不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet；
# 如果设置了 --hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；
# --experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；
# 管理员通过了 CSR 请求后，kubelet 自动在 --cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，然后写入 --kubeconfig 文件(自动创建 --kubeconfig 指定的文件)；
# 建议在 --kubeconfig 配置文件中指定 kube-apiserver 地址，如果未指定 --api-servers 选项，则必须指定 --require-kubeconfig 选项后才从配置文件中读取 kue-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息;
# --cluster-dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，--cluster-domain 指定域名后缀，这两个参数同时指定后才会生效；
# kubelet cAdvisor 默认在所有接口监听 4194 端口的请求，对于有外网的机器来说不安全，ExecStartPost 选项指定的 iptables 规则只允许内网机器访问 4194 端口；

sudo mkdir /var/lib/kubelet 
kubelet.service

cd /etc/kubernetes/
scp root@10.64.3.7:/etc/kubernetes/bootstrap.kubeconfig .
cd /root

#     启动 kubelet
sudo cp kubelet.service /etc/systemd/system/kubelet.service
sudo systemctl daemon-reload
sudo systemctl enable kubelet
sudo systemctl start kubelet
systemctl status kubelet


#   安装和配置 kubelet 1台 是的
kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
# --user=kubelet-bootstrap 是文件 /etc/kubernetes/token.csv 中指定的用户名，同时也写入了文件 /etc/kubernetes/bootstrap.kubeconfig


#     通过 kubelet 的 TLS 证书请求
# 查看未授权的 CSR 请求及节点

kubectl get csr
kubectl get nodes

kubectl certificate approve csr-x27td
kubectl certificate approve csr-srqwg

# 只有批准了才有端口出现,多个, 4194 

ls -l /etc/kubernetes/kubelet.kubeconfig
ls -l /etc/kubernetes/ssl/kubelet*
# 自己生成证书

kubectl get nodes

# 新的节点上线后,pod会自动同步上去,如何控制数量?

#   配置 kube-proxy
#     创建 kube-proxy 证书
cd /root/ssl
kube-proxy-csr.json

 cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
 -ca-key=/etc/kubernetes/ssl/ca-key.pem \
 -config=/etc/kubernetes/ssl/ca-config.json \
 -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy

 ls kube-proxy*
 sudo mv kube-proxy*.pem /etc/kubernetes/ssl/
 rm kube-proxy.csr  kube-proxy-csr.json

# CN 指定该证书的 User 为 system:kube-proxy；
# kube-apiserver 预定义的 RoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；
# hosts 属性值为空列表；


#     创建 kube-proxy kubeconfig 文件
# 设置集群参数
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/ssl/ca.pem \
--embed-certs=true \
--server=${KUBE_APISERVER} \
--kubeconfig=kube-proxy.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kube-proxy \
--client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
--client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
--embed-certs=true \
--kubeconfig=kube-proxy.kubeconfig

# 设置上下文参数
kubectl config set-context default \
--cluster=kubernetes \
--user=kube-proxy \
--kubeconfig=kube-proxy.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
mv kube-proxy.kubeconfig /etc/kubernetes/

# 这个还是每台要配置的,因为证书不一样

# 设置集群参数和客户端认证参数时 --embed-certs 都为 true，这会将 certificate-authority、client-certificate 和 client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；
# kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限

#     创建 kube-proxy 的 systemd unit 文件
sudo mkdir -p /var/lib/kube-proxy 

kube-proxy.service

# --hostname-override 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则；
# --cluster-cidr 必须与 kube-controller-manager 的 --cluster-cidr 选项值一致；
# kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；
# --kubeconfig 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息；
# 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；

#     启动 kube-proxy
sudo cp kube-proxy.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable kube-proxy
sudo systemctl start kube-proxy
systemctl status kube-proxy

#   验证集群功能
nginx-ds.yml
kubectl create -f nginx-ds.yml

#     检查节点状态
kubectl get nodes

#     检查各 Node 上的 Pod IP 连通性
kubectl get pods  -o wide|grep nginx-ds

#     检查服务 IP 和端口可达性
kubectl get svc |grep nginx-ds
# nginx-ds     10.254.64.28   <nodes>       80:8489/TCP   9m


 curl 10.254.136.178 # `kubectl get svc |grep nginx-ds` 输出中的服务 IP
# proxy安装成功才能访问svc上的机器

#     检查服务的 NodePort 可达性
export NODE_IP=10.64.3.7 # 当前 Node 的 IP
export NODE_PORT=8744 

# 上文 svc 输出中 80 端口映射的 NodePort

curl ${NODE_IP}:${NODE_PORT}
# ok




# 部署 DNS 插件
cd /root/kubernetes/cluster/addons/dns
ls *.yaml *.base

#   系统预定义的 RoleBinding
kubectl get clusterrolebindings system:kube-dns -o yaml

#   配置 kube-dns ServiceAccount
# 无需修改

#   配置 kube-dns 服务
diff kubedns-svc.yaml.base kubedns-svc.yaml
# CLUSTER_DNS_SVC_IP 10.254.0.2

#   配置 kube-dns Deployment
diff kubedns-controller.yaml.base kubedns-controller.yaml

#   执行所有定义文件
mkdir -p /root/kubernetes-git/cluster/addons/dns
cp kubedns-* /root/kubernetes-git/cluster/addons/dns
cd !$
kubectl create -f .
# configmap, deployment, serviceaccount, service

#   检查 kubedns 功能
my-nginx.yaml
kubectl create -f my-nginx.yaml

kubectl expose deploy my-nginx
kubectl get services --all-namespaces |grep my-nginx

pod-nginx.yaml
kubectl create -f pod-nginx.yaml
kubectl exec  nginx -i -t -- /bin/bash
root@nginx:/# cat /etc/resolv.conf

# ping my-nginx
# ping kubernetes
# ping kube-dns.kube-system.svc.cluster.local
exit 




# 部署 Dashboard 插件
cd kubernetes/cluster/addons/dashboard
# 定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定

#   配置dashboard-service
# diff dashboard-service.yaml.orig dashboard-service.yaml

#   配置dashboard-controller

#   执行所有定义文件
mkdir -p /root/kubernetes/cluster/addons/dashboard
mv dashboard-*  /root/kubernetes/cluster/addons/dashboard
cd !$
kubectl create -f  .

#   检查执行结果
kubectl get services kubernetes-dashboard -n kube-system
# kubernetes-dashboard   10.254.120.253   <nodes>       80:8749/TCP   39s
NodePort 8749映射到 dashboard pod 80端口

kubectl get deployment kubernetes-dashboard  -n kube-system
kubectl get pods  -n kube-system | grep dashboard

#   访问dashboard
#   通过 kubectl proxy 用新窗口访问 dashboard
kubectl proxy --address='10.64.3.7' --port=8086 --accept-hosts='^*$'
# Starting to serve on 10.64.3.7:8086

http://10.64.3.7:8086/ui
# 服务不可用
# 路径没有写入profile

#   通过 kube-apiserver 访问dashboard
kubectl cluster-info
http://10.64.3.7:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
# User "system:anonymous" cannot proxy services in the namespace "kube-system".
openssl pkcs12 -export -in admin.pem  -out admin.p12 -inkey admin-key.pem
# 失败,无法查看
kubectl describe pods kubernetes-dashboard-3677875397-lp5hv --namespace=kube-system
 # restart the nodes. (vagrant halt && vagrant up) restarting all minions




 # 部署 Heapster 插件
 #  配置 grafana-deployment
 #  配置 heapster-deployment
 #  配置 influxdb-deployment
 #  配置 monitoring-influxdb Service
 #  执行所有定义文件
 #  检查执行结果
 #  访问 grafana
 #  访问 influxdb admin UI

    # 10.64.3.7
    # 10.64.3.8
    # 10.66.3.86

    # Kubernetes 1.6.2
    # Docker 17.04.0-ce
    # Etcd 3.1.6
    # Flanneld 0.7.1 vxlan 网络
    # TLS 认证通信 (所有组件，如 etcd、kubernetes master 和 node)
    # RBAC 授权
    # kubelet TLS BootStrapping
    # kubedns、dashboard、heapster (influxdb、grafana)、EFK (elasticsearch、fluentd、kibana) 插件
    # 私有 docker registry，使用 ceph rgw 后端存储，TLS + HTTP Basic 认证

    # 对k8s来说, 主机名也是很重要的!
    # 没有定义好时间服务器, bin, 补全, 防火墙放行

# 补全
 yum install -y bash-completion
 locate bash_completion
 source /usr/share/bash-completion/bash_completion
 source <(kubectl completion bash)


    # 由10.64.3.7牵头
    touch environment.sh
    vi !$

    mkdir -p /root/local/bin
    cp environment.sh /root/local/bin
    # x3 

SERVICE_CIDR
CLUSTER_CIDR
# 重要,定义2个基础网络




    # 创建 CA 证书和秘钥
# CloudFlare 公司的一个 PKI工具
#   生成证书
#   构建Json生成证书
#   查看证书信息

yum -y install ntp && ntpdate -u cn.pool.ntp.org
systemctl enable ntpdate.service

    #   安装 CFSSL
    yum install wget -y
    wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
    chmod +x cfssl_linux-amd64
    sudo mv cfssl_linux-amd64 /root/local/bin/cfssl

    wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
    chmod +x cfssljson_linux-amd64
    sudo mv cfssljson_linux-amd64 /root/local/bin/cfssljson

    wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
    chmod +x cfssl-certinfo_linux-amd64
    sudo mv cfssl-certinfo_linux-amd64 /root/local/bin/cfssl-certinfo

cd /root
    export PATH=/root/local/bin:$PATH
    mkdir ssl
    cd ssl

# 生成模板

# 生成证书信息文件
    cfssl print-defaults config > config.json
    cfssl print-defaults csr > csr.json
# 模板及证书文件


    #   创建 CA
    # 创建 CA 配置文件

    ca-config.json
# ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；
# signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；
# server auth：表示 client 可以用该 CA 对 server 提供的证书进行验证；
# client auth：表示 server 可以用该 CA 对 client 提供的证书进行验证；

    # 创建 CA 证书签名请求
    ca-csr.json

    # 生成 CA 证书和私钥,唯一
cfssl gencert -initca ca-csr.json | cfssljson -bare ca

    #   分发证书
    sudo mkdir -p /etc/kubernetes/ssl

    sudo cp ca* /etc/kubernetes/ssl
    scp ca* root@10.66.3.86:/etc/kubernetes/ssl
    scp ca* root@10.64.3.8:/etc/kubernetes/ssl

    ll /etc/kubernetes/ssl

#   校验证书
    # 目前应该尚未生成,回头再验证

    #     使用 openssl 命令
    cd /etc/kubernetes/ssl/
    openssl x509 -noout -text -in kubernetes.pem
    # 过期 kubernetes.pem

    #     使用 cfssl-certinfo 命令
    cfssl-certinfo -cert kubernetes.pem

# 确认 Issuer 字段的内容和 ca-csr.json 一致；
# 确认 Subject 字段的内容和 kubernetes-csr.json 一致；
# 确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；
# 确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetes profile 一致



  # 部署高可用 Etcd 集群
  # etcd-host0：10.64.3.7
  # etcd-host1：10.64.3.8
  # etcd-host2：10.66.3.86

# 需要部署3次,部分为零时变量
#   使用的变量
    export NODE_NAME=etcd-host0 
    export NODE_IP=10.64.3.7
    export NODE_IPS="10.64.3.7 10.64.3.8 10.66.3.86" 
    export ETCD_NODES=etcd-host0=https://10.64.3.7:2380,etcd-host1=https://10.64.3.8:2380,etcd-host2=https://10.66.3.86:2380
    source /root/local/bin/environment.sh



    export NODE_NAME=etcd-host1 
    export NODE_IP=10.64.3.8
    export NODE_IPS="10.64.3.7 10.64.3.8 10.66.3.86" 
    export ETCD_NODES=etcd-host0=https://10.64.3.7:2380,etcd-host1=https://10.64.3.8:2380,etcd-host2=https://10.66.3.86:2380
    source /root/local/bin/environment.sh



    export NODE_NAME=etcd-host2 
    export NODE_IP=10.66.3.86
    export NODE_IPS="10.64.3.7 10.64.3.8 10.66.3.86" 
    export ETCD_NODES=etcd-host0=https://10.64.3.7:2380,etcd-host1=https://10.64.3.8:2380,etcd-host2=https://10.66.3.86:2380
    source /root/local/bin/environment.sh



#   下载二进制文件
    yum install -y wget
    wget https://github.com/coreos/etcd/releases/download/v3.1.6/etcd-v3.1.6-linux-amd64.tar.gz

    cd /root/
    scp root@10.64.3.7:/root/etcd-v3.1.6-linux-amd64.tar.gz .

    tar -xvf etcd-v3.1.6-linux-amd64.tar.gz
    sudo mv etcd-v3.1.6-linux-amd64/etcd* /root/local/bin

    #   创建 TLS 秘钥和证书
    # 分别创建 etcd 证书签名请求

    cd /root
    mkdir ssl
    cd ~/ssl
    export PATH=/root/local/bin:$PATH

    etcd-csr.json 

    # 生成 etcd 证书和私钥
    cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
    -ca-key=/etc/kubernetes/ssl/ca-key.pem \
    -config=/etc/kubernetes/ssl/ca-config.json \
    -profile=kubernetes etcd-csr.json | cfssljson -bare etcd

    ls etcd*

    # 这里生成了3套证书
    # kubernetes 和 etcd 证书中包含有使用该证书的 IP，所以需要对各节点分别生成，不能共享

    sudo mkdir -p /etc/etcd/ssl
    sudo mv etcd*.pem /etc/etcd/ssl
    rm etcd.csr  etcd-csr.json

    #   创建 etcd 的 systemd unit 文件
    sudo mkdir -p /var/lib/etcd 
    
    etcd.service
# 这是一个很好的做法,脚本不动, 改变部分用环境变量处理,下同
# ${NODE_IP}

# /var/lib/etcd
# etcd 的公私钥                 cert-file和key-file
# Peers 通信的公私钥和 CA 证书  peer-cert-file、peer-key-file、peer-trusted-ca-file
# 客户端的CA证书                trusted-ca-file

# --initial-cluster-state 值为 new 时，--name 的参数值必须位于 --initial-cluster 列表中；


    #   启动 etcd 服务
    firewall-cmd --zone=public --add-port=2379/tcp --permanent
    firewall-cmd --zone=public --add-port=2380/tcp --permanent
    firewall-cmd --reload

    # 注意这里要线性操作,一台好了再做一台

    sudo mv etcd.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable etcd

    sudo systemctl start etcd
    systemctl status etcd

    #   验证服务
    for ip in ${NODE_IPS}; do
      ETCDCTL_API=3 /root/local/bin/etcdctl \
      --endpoints=https://${ip}:2379  \
      --cacert=/etc/kubernetes/ssl/ca.pem \
      --cert=/etc/etcd/ssl/etcd.pem \
      --key=/etc/etcd/ssl/etcd-key.pem \
      endpoint health; done





# 下载和配置 Kubectl 命令行工具
    # kubectl 默认从 ~/.kube/config 配置文件获取访问 kube-apiserver 地址、证书、用户名等信息
ls ~/.kube/config

    # 需要将下载的 kubectl 二进制程序和生成的 ~/.kube/config 配置文件拷贝到所有使用 kubectl 命令的机器
    # 这样看来,配置应该是拷贝的,不知道证书怎么做,只要一个,还是分别生成?防火墙

    #   使用的变量
    export MASTER_IP=10.64.3.7
    export KUBE_APISERVER="https://${MASTER_IP}:6443"

    #   下载 kubectl x3
    # wget https://dl.k8s.io/v1.6.2/kubernetes-client-linux-amd64.tar.gz

    cd /root
    scp root@10.64.3.7:/root/kubernetes-client-linux-amd64.tar.gz .

    tar -xzvf kubernetes-client-linux-amd64.tar.gz
    sudo cp kubernetes/client/bin/kube* /root/local/bin/
    chmod a+x /root/local/bin/kube*
    export PATH=/root/local/bin:$PATH

    #   创建 admin 证书
    cd ssl
    admin-csr.json

    cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
      -ca-key=/etc/kubernetes/ssl/ca-key.pem \
      -config=/etc/kubernetes/ssl/ca-config.json \
      -profile=kubernetes admin-csr.json | cfssljson -bare admin

    ls admin*
    sudo mv admin*.pem /etc/kubernetes/ssl/
    rm admin.csr admin-csr.json

# kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权；

# kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，
# 如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 所有 API的权限；

# 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，
# 同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限；

# hosts 属性值为空列表；


    #   创建 kubectl kubeconfig 文件
    # 设置集群参数
    kubectl config set-cluster kubernetes \
    --certificate-authority=/etc/kubernetes/ssl/ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER}

    # 设置客户端认证参数
    kubectl config set-credentials admin \
    --client-certificate=/etc/kubernetes/ssl/admin.pem \
    --embed-certs=true \
    --client-key=/etc/kubernetes/ssl/admin-key.pem

    # 设置上下文参数
    kubectl config set-context kubernetes \
    --cluster=kubernetes \
    --user=admin

    # 设置默认上下文
    kubectl config use-context kubernetes

    #   分发 kubeconfig 文件
    # 将 ~/.kube/config 文件拷贝到运行 kubelet 命令的机器的 ~/.kube/ 目录下

    # 三段配置+激活,生成文件写有密钥

# admin.pem 证书 O 字段值为 system:masters，
# kube-apiserver 预定义的 RoleBinding cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，
# 该 Role 授予了调用kube-apiserver 相关 API 的权限；

# 生成的 kubeconfig 被保存到 ~/.kube/config 文件

    mkdir .kube
    cd !$
    scp root@10.64.3.7:/root/.kube/config .
    # 使用 kubectl  的机器都需要




    # 配置 Flannel 网络 三台都要
    # kubernetes 要求集群内各节点能通过 Pod 网段互联互通，使用 Flannel 在所有节点 (Master、Node) 上创建互联互通的 Pod 网段
    #   使用的变量 
    export NODE_IP=10.64.3.7
    source /root/local/bin/environment.sh

    # 我猜是只需要在在一台机器上操作,后续观察

    #   创建 TLS 秘钥和证书 
    # etcd 集群启用了双向 TLS 认证，所以需要为 flanneld 指定与 etcd 集群通信的 CA 和秘钥
    # 创建 flanneld 证书签名请求
    cd ssl
    flanneld-csr.json

     cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
     -ca-key=/etc/kubernetes/ssl/ca-key.pem \
     -config=/etc/kubernetes/ssl/ca-config.json \
     -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld

     ls flanneld*
     sudo mkdir -p /etc/flanneld/ssl
     sudo mv flanneld*.pem /etc/flanneld/ssl
     rm flanneld.csr  flanneld-csr.json

     # 证书里ip倒是没有,先分别建吧

    #   向 etcd 写入集群 Pod 网段信息
# 只需在第一次部署 Flannel 网络时执行，后续在其它节点上部署 Flannel 时无需再写入该信息！

    /root/local/bin/etcdctl \
      --endpoints=${ETCD_ENDPOINTS} \
      --ca-file=/etc/kubernetes/ssl/ca.pem \
      --cert-file=/etc/flanneld/ssl/flanneld.pem \
      --key-file=/etc/flanneld/ssl/flanneld-key.pem \
      set ${FLANNEL_ETCD_PREFIX}/config '{"Network":"'${CLUSTER_CIDR}'", "SubnetLen": 24, "Backend": {"Type": "vxlan"}}'

    #   安装和配置 flanneld
    #     下载 flanneld
    cd ~
    mkdir flannel

    # wget https://github.com/coreos/flannel/releases/download/v0.7.1/flannel-v0.7.1-linux-amd64.tar.gz

    scp root@10.64.3.7:/root/flannel-v0.7.1-linux-amd64.tar.gz .

    tar -xzvf flannel-v0.7.1-linux-amd64.tar.gz -C flannel
    sudo cp flannel/{flanneld,mk-docker-opts.sh} /root/local/bin
    #     创建 flanneld 的 systemd unit 文件

    # 检查网络环境
      # mk-docker-opts.sh
      # --iface=ens33
    flanneld.service

# /run/flannel/docker
# --iface 选项值指定通信接口

    #     启动 flanneld
    sudo cp flanneld.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable flanneld

    # 集群的东西,建议还是一个一个启动好

    sudo systemctl start flanneld
    systemctl status flanneld

    #     检查 flanneld 服务
    journalctl  -u flanneld |grep 'Lease acquired'
    ifconfig flannel.1

    172.30.95.0/32 7
    172.30.99.0/32 8
    172.30.101.0/32 86

    #   检查分配给各 flanneld 的 Pod 网段信息
     # 查看集群 Pod 网段(/16)
     /root/local/bin/etcdctl \
     --endpoints=${ETCD_ENDPOINTS} \
     --ca-file=/etc/kubernetes/ssl/ca.pem \
     --cert-file=/etc/flanneld/ssl/flanneld.pem \
     --key-file=/etc/flanneld/ssl/flanneld-key.pem \
     get ${FLANNEL_ETCD_PREFIX}/config
     # { "Network": "172.30.0.0/16", "SubnetLen": 24, "Backend": { "Type": "vxlan" } }

     # 查看已分配的 Pod 子网段列表(/24)
     /root/local/bin/etcdctl \
     --endpoints=${ETCD_ENDPOINTS} \
     --ca-file=/etc/kubernetes/ssl/ca.pem \
     --cert-file=/etc/flanneld/ssl/flanneld.pem \
     --key-file=/etc/flanneld/ssl/flanneld-key.pem \
     ls ${FLANNEL_ETCD_PREFIX}/subnets
    # /kubernetes/network/subnets/172.30.19.0-24

     # 查看某一 Pod 网段对应的 flanneld 进程监听的 IP 和网络参数
     /root/local/bin/etcdctl \
     --endpoints=${ETCD_ENDPOINTS} \
     --ca-file=/etc/kubernetes/ssl/ca.pem \
     --cert-file=/etc/flanneld/ssl/flanneld.pem \
     --key-file=/etc/flanneld/ssl/flanneld-key.pem \
     get ${FLANNEL_ETCD_PREFIX}/subnets/172.30.18.0-24
    # {"PublicIP":"10.64.3.7","BackendType":"vxlan","BackendData":{"VtepMAC":"d6:51:2e:80:5c:69"}}

    #   确保各节点间 Pod 网段能互联互通
    /root/local/bin/etcdctl \
      --endpoints=${ETCD_ENDPOINTS} \
      --ca-file=/etc/kubernetes/ssl/ca.pem \
      --cert-file=/etc/flanneld/ssl/flanneld.pem \
      --key-file=/etc/flanneld/ssl/flanneld-key.pem \
      ls ${FLANNEL_ETCD_PREFIX}/subnets
    # /kubernetes/network/subnets/172.30.99.0-24
    # /kubernetes/network/subnets/172.30.101.0-24
    # /kubernetes/network/subnets/172.30.95.0-24


    # iptables -A FORWARD -s 172.30.0.0/16 -j ACCEPT
    # firewall-cmd --add-rich-rule="rule family='ipv4' source address='172.30.0.0/16' accept"
    # 两边都没有说明,先不开防火墙吧
    # ping 172.30.18.1
    # ping 172.30.16.2
    # ping 172.30.39.3
    # 在各节点上分配 ping 这三个网段的网关地址，确保能通
    # 不是很明白

    cat /run/flannel/docker
    # 怀疑重启etcd挂了




    # 部署 Master 节点

# kube-apiserver
# kube-scheduler
# kube-controller-manager

# kube-scheduler、kube-controller-manager 和 kube-apiserver 三者的功能紧密相关；
# 同时只能有一个 kube-scheduler、kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader

# master 节点与 node 节点上的 Pods 通过 Pod 网络通信，所以需要在 master 节点上部署 Flannel 网络


    #   使用的变量
    export MASTER_IP=10.64.3.7
    source /root/local/bin/environment.sh

    #   下载最新版本的二进制文件,下面一种好一点

    wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.2/kubernetes.tar.gz

    tar -xzvf kubernetes.tar.gz
    cd kubernetes
    ./cluster/get-kube-binaries.sh
    # 先布一台, 从googleapis下载

    # https://github.com/kubernetes/kubernetes/releases
    # https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md

    # # wget https://dl.k8s.io/v1.6.2/kubernetes-client-linux-amd64.tar.gz
    wget https://dl.k8s.io/v1.6.2/kubernetes-server-linux-amd64.tar.gz
    # tar -xzvf kubernetes-server-linux-amd64.tar.gz
    # cd kubernetes
    # tar -xzvf  kubernetes-src.tar.gz
    # 要解压下载下来的server包

    sudo cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /root/local/bin/

    # 没有文件

    #   安装和配置 flanneld
    #   创建 kubernetes 证书
    cd ~/ssl
    kubernetes-csr.json

# 如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，分别指定当前部署的 master 节点主机 IP；

# 还需要添加 kube-apiserver 注册的名为 kubernetes 的服务 IP (Service Cluster IP)，
# 一般是 kube-apiserver --service-cluster-ip-range 选项值指定的网段的第一个IP，如 "10.254.0.1"；


    firewall-cmd --zone=public --add-port=6443/tcp --permanent
    firewall-cmd --reload

    # kubectl get svc kubernetes 后期

    cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
    -ca-key=/etc/kubernetes/ssl/ca-key.pem \
    -config=/etc/kubernetes/ssl/ca-config.json \
    -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes

    ls kubernetes*
    sudo mkdir -p /etc/kubernetes/ssl/
    sudo mv kubernetes*.pem /etc/kubernetes/ssl/
    rm kubernetes.csr  kubernetes-csr.json


    #   配置和启动 kube-apiserver

    # kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求
    # kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token.csv 一致，
    # 如果一致则自动为 kubelet生成证书和秘钥

    #     创建 kube-apiserver 使用的客户端 token 文件
    cat > token.csv <<EOF
    ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
    EOF

    mv token.csv /etc/kubernetes/
    #     创建 kube-apiserver 的 systemd unit 文件

    kube-apiserver.service

# kube-apiserver 1.6 版本开始使用 etcd v3 API 和存储格式；

# 指定在安全端口使用 RBAC 授权模式，拒绝未通过授权的请求；
--authorization-mode=RBAC 

# kube-scheduler、kube-controller-manager 一般和 kube-apiserver 部署在同一台机器上，它们使用非安全端口和 kube-apiserver通信;

# kubelet、kube-proxy、kubectl 部署在其它 Node 节点上，
# 如果通过安全端口访问 kube-apiserver，则必须先通过 TLS 证书认证，再通过 RBAC 授权；

# kube-proxy、kubectl 通过在使用的证书里指定相关的 User、Group 来达到通过 RBAC 授权的目的；

# 如果使用了 kubelet TLS Boostrap 机制，
# 则不能再指定 --kubelet-certificate-authority、--kubelet-client-certificate 和 --kubelet-client-key 选项，
# 否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误；

# --admission-control 值必须包含 ServiceAccount，否则部署集群插件时会失败；
# --bind-address 不能为 127.0.0.1；
# --service-cluster-ip-range 指定 Service Cluster IP 地址段，该地址段不能路由可达；
# --service-node-port-range=${NODE_PORT_RANGE} 指定 NodePort 的端口范围；

# 缺省情况下 kubernetes 对象保存在 etcd /registry 路径下，可以通过 --etcd-prefix 参数进行调整

# 有很多参数,根据需要仔细调节,不过可以做模板化,只改变量

    #     启动 kube-apiserver
    sudo cp kube-apiserver.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable kube-apiserver
    sudo systemctl start kube-apiserver
    sudo systemctl status kube-apiserver





    #   配置和启动 kube-controller-manager
    #     创建 kube-controller-manager 的 systemd unit 文件

    kube-controller-manager.service
    kubectl get componentstatuses
    # --address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器
# --master=http://{MASTER_IP}:8080：使用非安全 8080 端口与 kube-apiserver 通信；

# --cluster-cidr 指定 Cluster 中 Pod 的 CIDR 范围，该网段在各 Node 间必须路由可达(flanneld保证)；

# --service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致；

# --cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥；

# --root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件；

# --leader-elect=true 部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程；

    #     启动 kube-controller-manager
    sudo cp kube-controller-manager.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable kube-controller-manager
    sudo systemctl start kube-controller-manager



    #   配置和启动 kube-scheduler
    #     创建 kube-scheduler 的 systemd unit 文件
    kube-scheduler.service

# --address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器；
# --master=http://{MASTER_IP}:8080：使用非安全 8080 端口与 kube-apiserver 通信；
# --leader-elect=true 部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程

    #     启动 kube-scheduler
    sudo cp kube-scheduler.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable kube-scheduler
    sudo systemctl start kube-scheduler

    #   验证 master 节点功能
    kubectl get componentstatuses




# 部署 Node 节点
  # 感觉master有3台,节点有多个,后面验证

    #   使用的变量
    export MASTER_IP=10.64.3.7
    export KUBE_APISERVER="https://${MASTER_IP}:6443"
    export NODE_IP=10.64.3.7
    source /root/local/bin/environment.sh

    # 第二台
    # 替换为 kubernetes master 集群任一机器 IP
    export MASTER_IP=10.64.3.7
    export KUBE_APISERVER="https://${MASTER_IP}:6443"
    export NODE_IP=10.64.3.8
    source /root/local/bin/environment.sh

    #   安装和配置 flanneld
    #   安装和配置 docker
    #     下载最新的 docker 二进制文件
    # wget https://get.docker.com/builds/Linux/x86_64/docker-17.04.0-ce.tgz

    scp root@10.64.3.7:/root/docker-17.04.0-ce.tgz .

    tar -xvf docker-17.04.0-ce.tgz
    cp docker/docker* /root/local/bin
    cp docker/completion/bash/docker /etc/bash_completion.d/

    # dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中；

    # flanneld 启动时将网络配置写入到 /run/flannel/docker 文件中的变量 DOCKER_NETWORK_OPTIONS，
    # dockerd 命令行上指定该变量值来设置 docker0 网桥参数；

    # 如果指定了多个 EnvironmentFile 选项，则必须将 /run/flannel/docker 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；

    # 不能关闭默认开启的 --iptables 和 --ip-masq 选项；

    # 如果内核版本比较新，建议使用 overlay 存储驱动


    #     创建 docker 的 systemd unit 文件
    cd ~

    docker.service

    sudo iptables -P FORWARD ACCEPT

    vi /etc/rc.local
      sleep 60 && /sbin/iptables -P FORWARD ACCEPT

    # 防火墙的配置先,放一下回头补充,注意要处理3台

    mkdir /etc/docker/
    cd !$
    cat > daemon.json <<EOF
    {
      "registry-mirrors": ["https://docker.mirrors.ustc.edu.cn", "hub-mirror.c.163.com"],
      "max-concurrent-downloads": 10
    }
    EOF

# registry.cn-hangzhou.aliyuncs.com

    #     启动 dockerd
    cd ~
    sudo cp docker.service /etc/systemd/system/docker.service
    sudo systemctl daemon-reload

    sudo systemctl stop firewalld
    sudo systemctl disable firewalld
    sudo iptables -F && sudo iptables -X && sudo iptables -F -t nat && sudo iptables -X -t nat
    
    sudo systemctl enable docker
    sudo systemctl start docker

    #     检查 docker 服务
    docker version

    # kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，
    # 需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，
    # 然后 kubelet 才有权限创建认证请求(certificatesigningrequests)


参数配置错误
cd /etc/systemd/system/
vi docker.service

sudo systemctl stop docker
systemctl daemon-reload
sudo systemctl start docker
sudo systemctl status docker


    # #     下载最新的 kubelet 和 kube-proxy 二进制文件 master已有
    # wget https://dl.k8s.io/v1.6.2/kubernetes-server-linux-amd64.tar.gz
    # tar -xzvf kubernetes-server-linux-amd64.tar.gz
    # cd kubernetes
    cd /root/kubernetes/server/kubernetes
    # 从master上copy
    # tar -xzvf  kubernetes-src.tar.gz
    sudo cp -r ./server/bin/{kube-proxy,kubelet} /root/local/bin/

    /root/local/bin/
    scp root@10.64.3.7:/root/local/bin/{kube-proxy,kubelet} .


    #     创建 kubelet bootstrapping kubeconfig 文件
    # 感觉也不需要每台都设,待确认
    # 应该是要的,对服务设的

    # --embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；
    # 设置 kubelet 客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成

    cd /root

    # 设置集群参数
    kubectl config set-cluster kubernetes \
    --certificate-authority=/etc/kubernetes/ssl/ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=bootstrap.kubeconfig

    # 设置客户端认证参数
    kubectl config set-credentials kubelet-bootstrap \
    --token=${BOOTSTRAP_TOKEN} \
    --kubeconfig=bootstrap.kubeconfig

    # 设置上下文参数
    kubectl config set-context default \
    --cluster=kubernetes \
    --user=kubelet-bootstrap \
    --kubeconfig=bootstrap.kubeconfig

    # 设置默认上下文
    kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
    mv bootstrap.kubeconfig /etc/kubernetes/

    # 理论上只要做一次,后面尝试,不能用再添加
    # 是的,只有第一次配置

    #     创建 kubelet 的 systemd unit 文件
    # --address 不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet；
    # 如果设置了 --hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；
    # --experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；
    # 管理员通过了 CSR 请求后，kubelet 自动在 --cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，然后写入 --kubeconfig 文件(自动创建 --kubeconfig 指定的文件)；
    # 建议在 --kubeconfig 配置文件中指定 kube-apiserver 地址，如果未指定 --api-servers 选项，则必须指定 --require-kubeconfig 选项后才从配置文件中读取 kue-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息;
    # --cluster-dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，--cluster-domain 指定域名后缀，这两个参数同时指定后才会生效；
    # kubelet cAdvisor 默认在所有接口监听 4194 端口的请求，对于有外网的机器来说不安全，ExecStartPost 选项指定的 iptables 规则只允许内网机器访问 4194 端口；

    sudo mkdir /var/lib/kubelet 
    kubelet.service

# --address 不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，
# 因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet；

# 如果设置了 --hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；

# --experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，
# kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；

# 管理员通过了 CSR 请求后，kubelet 自动在 --cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，
# 然后写入 --kubeconfig 文件(自动创建 --kubeconfig 指定的文件)；

# 建议在 --kubeconfig 配置文件中指定 kube-apiserver 地址，
# 如果未指定 --api-servers 选项，则必须指定 --require-kubeconfig 选项后才从配置文件中读取 kue-apiserver 的地址，
# 否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息;

# --cluster-dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，
# --cluster-domain 指定域名后缀，这两个参数同时指定后才会生效；

# kubelet cAdvisor 默认在所有接口监听 4194 端口的请求，对于有外网的机器来说不安全，
# ExecStartPost 选项指定的 iptables 规则只允许内网机器访问 4194 端口；


    cd /etc/kubernetes/
    scp root@10.64.3.7:/etc/kubernetes/bootstrap.kubeconfig .
    cd /root

    #     启动 kubelet
    sudo cp kubelet.service /etc/systemd/system/kubelet.service
    sudo systemctl daemon-reload
    sudo systemctl enable kubelet
    sudo systemctl start kubelet
    systemctl status kubelet


    #   安装和配置 kubelet 1台 是的
    kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
    # --user=kubelet-bootstrap 是文件 /etc/kubernetes/token.csv 中指定的用户名，同时也写入了文件 /etc/kubernetes/bootstrap.kubeconfig


    #     通过 kubelet 的 TLS 证书请求
    # 查看未授权的 CSR 请求及节点

    kubectl get csr
    kubectl get nodes

    kubectl certificate approve csr-x27td
    kubectl certificate approve csr-srqwg

    # 只有批准了才有端口出现,多个, 4194 

    ls -l /etc/kubernetes/kubelet.kubeconfig
    ls -l /etc/kubernetes/ssl/kubelet*
    # 自己生成证书

    kubectl get nodes

    # 新的节点上线后,pod会自动同步上去,如何控制数量?

    #   配置 kube-proxy
    #     创建 kube-proxy 证书
    cd /root/ssl
    kube-proxy-csr.json

    # CN 指定该证书的 User 为 system:kube-proxy；
    # kube-apiserver 预定义的 RoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，
    # 该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；

    # hosts 属性值为空列表；

     cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
     -ca-key=/etc/kubernetes/ssl/ca-key.pem \
     -config=/etc/kubernetes/ssl/ca-config.json \
     -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy

     ls kube-proxy*
     sudo mv kube-proxy*.pem /etc/kubernetes/ssl/
     rm kube-proxy.csr  kube-proxy-csr.json

    # CN 指定该证书的 User 为 system:kube-proxy；
    # kube-apiserver 预定义的 RoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；
    # hosts 属性值为空列表；


    #     创建 kube-proxy kubeconfig 文件
    # 设置集群参数
    kubectl config set-cluster kubernetes \
    --certificate-authority=/etc/kubernetes/ssl/ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=kube-proxy.kubeconfig

    # 设置客户端认证参数
    kubectl config set-credentials kube-proxy \
    --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
    --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
    --embed-certs=true \
    --kubeconfig=kube-proxy.kubeconfig

    # 设置上下文参数
    kubectl config set-context default \
    --cluster=kubernetes \
    --user=kube-proxy \
    --kubeconfig=kube-proxy.kubeconfig

    # 设置默认上下文
    kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
    mv kube-proxy.kubeconfig /etc/kubernetes/

    # 这个还是每台要配置的,因为证书不一样

    # 设置集群参数和客户端认证参数时 --embed-certs 都为 true，
    # 这会将 certificate-authority、client-certificate 和 client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；

    # kube-proxy.pem 证书中 CN 为 system:kube-proxy，
    # kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，
    # 该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限

    #     创建 kube-proxy 的 systemd unit 文件
    sudo mkdir -p /var/lib/kube-proxy 

    kube-proxy.service

    # --hostname-override 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则；
    # --cluster-cidr 必须与 kube-controller-manager 的 --cluster-cidr 选项值一致；
    # kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；
    # --kubeconfig 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息；
    # 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；

    #     启动 kube-proxy
    sudo cp kube-proxy.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable kube-proxy
    sudo systemctl start kube-proxy
    systemctl status kube-proxy

    #   验证集群功能
    nginx-ds.yml
    kubectl create -f nginx-ds.yml

    #     检查节点状态
    kubectl get nodes

    #     检查各 Node 上的 Pod IP 连通性
    kubectl get pods  -o wide|grep nginx-ds

    #     检查服务 IP 和端口可达性
    kubectl get svc |grep nginx-ds
    # nginx-ds     10.254.64.28   <nodes>       80:8489/TCP   9m


     curl 10.254.136.178 # `kubectl get svc |grep nginx-ds` 输出中的服务 IP
    # proxy安装成功才能访问svc上的机器

    #     检查服务的 NodePort 可达性
    export NODE_IP=10.64.3.7 # 当前 Node 的 IP
    export NODE_PORT=8744 

    # 上文 svc 输出中 80 端口映射的 NodePort

    curl ${NODE_IP}:${NODE_PORT}
    # ok

# 自定义输出结果
kubectl get pods --all-namespaces -o go-template='{{range .items}}{{.metadata.uid}}{{end}}'
kubectl get pods --all-namespaces -o go-template --template='{{range .items}}{{printf "|%-20s|%-50s|%-30s|\n" .metadata.namespace .metadata.name .metadata.uid}}{{end}}'
kubectl get pods --all-namespaces -o go-template --template='{{range .items}}{{range .spec.containers}}{{printf "%s\n" .image}}{{end}}{{end}}'
# 列出所有不可调度节点的节点名与 IP
kubectl get no -o go-template='{{range .items}}{{if .spec.unschedulable}}{{.metadata.name}} {{.spec.externalID}}{{"\n"}}{{end}}{{end}}'
kubectl -n kube-system get pods coredns-64b597b598-7547d -o custom-columns=NAME:.metadata.name,hostip:.status.hostIP

cat > test.tmpl << EOF 
NAME                      HOSTIP
metadata.name       status.hostIP
EOF

kubectl -n kube-system get pods coredns-64b597b598-7547d -o custom-columns-file=test.tmpl

# 内部通过clusterip方式访问，外部通过nodeport方式访问


    # 部署 DNS 插件
    cd /root/kubernetes/cluster/addons/dns
    ls *.yaml *.base

    # 理论上一个就够了?

    #   系统预定义的 RoleBinding
    kubectl get clusterrolebindings system:kube-dns -o yaml

    #   配置 kube-dns ServiceAccount
    # 无需修改



    #   配置 kube-dns 服务
    diff kubedns-svc.yaml.base kubedns-svc.yaml
    # CLUSTER_DNS_SVC_IP 10.254.0.2

    #   配置 kube-dns Deployment
    kubedns-controller.yaml
    # 58 镜像
    # 88 域名
    # 92 注释 __PILLAR__FEDERATIONS__DOMAIN__MAP__
    # 110 镜像
    # 129 修改域名
    # 148 镜像
    # 161 域名

# --domain 为集群环境文档 变量 CLUSTER_DNS_DOMAIN 的值 注意
# 使用系统已经做了 RoleBinding 的 kube-dns ServiceAccount，该账户具有访问 kube-apiserver DNS 相关 API 的权限

    #   执行所有定义文件
    mkdir -p /root/kubernetes-git/cluster/addons/dns
    cp kubedns-* /root/kubernetes-git/cluster/addons/dns
    cd !$
    kubectl create -f .

    # configmap "kube-dns" created
    # deployment "kube-dns" created
    # serviceaccount "kube-dns" created
    # service "kube-dns" created
    # 分别是 设置,部署,帐号,服务

    #   检查 kubedns 功能

    my-nginx.yaml
    # 简历配置

    kubectl create -f my-nginx.yaml
    # 部署配置

    kubectl expose deploy my-nginx
    kubectl get services --all-namespaces |grep my-nginx
    # 暴露为服务,检查


    # 创建另一个 Pod，
    # 查看 /etc/resolv.conf 是否包含 kubelet 配置的 --cluster-dns 和 --cluster-domain，
    # 是否能够将服务 my-nginx 解析到上面显示的 Cluster IP 10.254.86.48

    pod-nginx.yaml
    kubectl create -f pod-nginx.yaml

    kubectl exec  nginx -i -t -- /bin/bash
    root@nginx:/# cat /etc/resolv.conf

    ping my-nginx
    ping kubernetes
    ping kube-dns.kube-system.svc.cluster.local
    exit 

    # 进入docker中, 测试dns, 结果失败,端口没开,怀疑镜像不行了
    # 如何查看镜像及docker日志?
    # pods没起来
# NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
# kube-dns               1         0         0            0           2h

# 重装了一下好了,怪事

# Failed to pull image "rosskukulinski/dne:v1.0.0": rpc error: code = 2 desc = Error response from daemon: {"message":"repository rosskukulinski/dne not found: does not exist or no pull access"}
# Error syncing pod, skipping: failed to "StartContainer" for "fail" with ErrImagePull: "rpc error: code = 2 desc = Error response from daemon: {\"message\":\"repository rosskukulinski/dne not found: does not exist or no pull access\"}"

# k8s将Service的名称当做域名注册到kube-dns中，通过Service的名称就可以访问其提供的服务

iptables -P FORWARD ACCEPT

# https://github.com/opsnull/follow-me-install-kubernetes-cluster/issues/139
# 正常状态是, 跨node pod互通, 每个node上有一个dns ? dns的pod由3个容器组成

kubectl exec -n kube-system -ti kube-dns-v20-xxxxx -c healthz -- nslookup kube-dns.kube-system.svc.cluster.local
kubectl exec -n kube-system -ti kube-dns-v20-xxxxx -c healthz -- nslookup kube-dns


# 集群内访问集群外
    # 集群内的Pod会继承Node上的DNS解析规则
    # ExternalName类型的Service，可以与一个公网域名绑定，通过该Service可以访问对应公网服务

# 集群外访问集群内
    # Service设置为NodePort类型 Node的IP + Service Port
    # 通过kube-proxy可以对外暴露集群内的服务
    # 根据实际情况在集群内自定义实现反向代理,?


# 部署 Dashboard 插件
    cd /root/kubernetes/cluster/addons/dashboard
    # 定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定

    #   配置dashboard-service
    # diff dashboard-service.yaml.orig dashboard-service.yaml

    dashboard-controller.yaml   
    # 添加 serviceAccountName: dashboard
    # 更换image

    dashboard-rbac.yaml
    # 新建,解决rbac认证

    dashboard-service.yaml
    # 指定端口类型为 NodePort

    #   配置dashboard-controller

    #   执行所有定义文件
    mkdir -p /root/kubernetes/cluster/addons/dashboard
    mv dashboard-*  /root/kubernetes/cluster/addons/dashboard
    cd !$
    kubectl create -f  .

    # deployment "kubernetes-dashboard" created
    # serviceaccount "dashboard" created
    # clusterrolebinding "dashboard" created
    # service "kubernetes-dashboard" created


    #   检查执行结果
    kubectl get services kubernetes-dashboard -n kube-system
    # kubernetes-dashboard   10.254.120.253   <nodes>       80:8749/TCP   39s
    # NodePort 8749映射到 dashboard pod 80端口

    kubectl get deployment kubernetes-dashboard  -n kube-system
    kubectl get pods  -n kube-system | grep dashboard

    #   访问dashboard
    #   通过 kubectl proxy 用新窗口访问 dashboard
    kubectl proxy --address='10.64.3.7' --port=8086 --accept-hosts='^*$'
    # Starting to serve on 10.64.3.7:8086
    # --accept-hosts 必须 否则Unauthorized

    http://10.64.3.7:8086/ui
    # 成功, 需要保持 proxy 程序开启


    #   通过 kube-apiserver 访问dashboard
    kubectl cluster-info
    http://10.64.3.7:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
    # 成功

    openssl pkcs12 -export -in admin.pem  -out admin.p12 -inkey admin-key.pem
    # 失败,无法查看, 不明

kubectl describe pods kubernetes-dashboard-3677875397-gv1s4 --namespace=kube-system
     # restart the nodes. (vagrant halt && vagrant up) restarting all minions

    # 缺少 Heapster 插件，当前 dashboard 不能展示 Pod、Nodes 的 CPU、内存等 metric 图形





 # 部署 Heapster 插件
# 1.2 移除,本节跳过

yum install unzip -y
wget https://github.com/kubernetes/heapster/archive/v1.3.0.zip
unzip v1.3.0.zip
mv v1.3.0.zip heapster-1.3.0

 #  配置 grafana-deployment

cd /root/heapster-1.3.0/deploy/kube-config/influxdb
ls *.yaml

heapster-rbac.yaml
influxdb-cm.yaml
# 很长的配置,有机会学习一下


grafana-deployment.yaml
# 如果后续使用 kube-apiserver 或者 kubectl proxy 访问 grafana dashboard，
# 必须将 GF_SERVER_ROOT_URL 设置为 /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/



 #  配置 heapster-deployment
 heapster-deployment.yaml
# 使用的是自定义的、名为 heapster 的 ServiceAccount

 #  配置 influxdb-deployment

# influxdb 官方建议使用命令行或 HTTP API 接口来查询数据库，从 v1.1.0 版本开始默认关闭 admin UI，
# 将在后续版本中移除 admin UI 插件
 
 # 开启镜像中 admin UI的办法如下：先导出镜像中的 influxdb 配置文件，开启 admin 插件后，
 # 再将配置文件内容写入 ConfigMap，最后挂载到镜像中

# manifests 目录下的 ConfigMap 文件

# 导出镜像中的 influxdb 配置文件
docker run --rm --entrypoint 'cat'  -ti lvanneo/heapster-influxdb-amd64:v1.1.1 /etc/config.toml >config.toml.orig
cp config.toml.orig config.toml

# 修改：启用 admin 接口
vi config.toml

# 将修改后的配置写入到 ConfigMap 对象中
kubectl create configmap influxdb-config --from-file=config.toml  -n kube-system

influxdb-deployment.yaml
#   - mountPath: /etc/
#     name: influxdb-config

# - name: influxdb-config
#   configMap:
#     name: influxdb-config



 #  配置 monitoring-influxdb Service

 influxdb-service.yaml
 # type: NodePort

  # name: http
# - port: 8083
  # targetPort: 8083
  # name: admin

# deployment "monitoring-grafana" created
# service "monitoring-grafana" created
# deployment "heapster" created
# serviceaccount "heapster" created
# clusterrolebinding "heapster" created
# service "heapster" created
# deployment "monitoring-influxdb" created
# service "monitoring-influxdb" created

# Error from server (AlreadyExists): error when creating "influxdb-cm.yaml": configmaps "influxdb-config" already exists

 #  执行所有定义文件

cd /root/heapster-1.3.0/deploy/kube-config/influxdb
kubectl create -f  .

 #  检查执行结果
 kubectl get deployments -n kube-system | grep -E 'heapster|monitoring'
 kubectl get pods -n kube-system | grep -E 'heapster|monitoring'

 #  访问 grafana
 kubectl cluster-info
http://10.64.3.7:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana

kubectl proxy --address='10.64.3.7' --port=8086 --accept-hosts='^*$'
http://10.64.3.7:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana


 #  访问 influxdb admin UI
kubectl get svc -n kube-system|grep influxdb
http://10.64.3.7:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8083/

# 这一章,熟练度不够,回头找到文件还要再熟悉下
# 部署完成,测试全失败

# Failed to list *api.Namespace: the server does not allow access to the requested resource (get namespaces)

# 配置数据源
# 名字,类型, http配置,http 认证,InfluxDB Details 库名, 用户密码

# 创建graph，切换编辑模式“Toggle Edit Mode”, 然后输入自定义SQL查询

SELECT derivative("value") AS "value" FROM "interface_rx" WHERE "host" = 'k8sslave04' AND "type" = 'if_octets' AND "instance" = 'eno16777984'
# 函数 derivative 意为导数, 微积分中的概念

# 系统负载
SELECT mean("value") FROM "load_longterm" WHERE "host" = 'k8sslave04' AND $timeFilter GROUP BY time($interval) fill(null)
SELECT mean("value") FROM "load_midterm" WHERE "host" = 'k8sslave04' AND $timeFilter GROUP BY time($interval) fill(null)
SELECT mean("value") FROM "load_shortterm" WHERE "host" = 'k8sslave04' AND $timeFilter GROUP BY time($interval) fill(null)

# 内存用量
SELECT mean("value") FROM "memory_value" WHERE "type_instance" = 'used' AND $timeFilter GROUP BY time($interval) fill(null)

# RestAPI操作
# 创建数据库
curl -i -XPOST http://influxdb-ip:8086/query --data-urlencode "q=CREATE DATABASE test"

# 写入单条数据
curl -i -XPOST 'http://influxdb-ip:8086/write?db=test' --data-binary 'cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000'
# 数据由度量cpu_load_short，标签键host和region和对应的标签值server01和us-west，
# 字段值为0.64的字段键value和时间戳1434055562000000000组成

# 将cpu_data.txt中数据写入mydb数据库
curl -i -XPOST 'http://influxdb-ip:8086/write?db=test' --data-binary @cpu_data.txt
# https://blog.csdn.net/liukuan73/article/details/79950329

increase(com_codahale_metrics_servlet_InstrumentedFilter_requests_count{project="$project"}[1m])
avg(com_codahale_metrics_servlet_InstrumentedFilter_requests{project="$project"} * 1000) by (quantile) 
# 使用编辑可以操作,但这每个都是自己写的么

curl  http://10.64.3.7/api/v1/model/namespaces/default/pod-list/busybox,busybox1/metrics/cpu/usage_rate
{"items":[{"metrics":[{"timestamp":"2017-05-05T01:36:00Z","value":0},{"timestamp":"2017-05-05T01:37:00Z","value":0},{"timestamp":"2017-05-05T01:38:00Z","value":0},{"timestamp":"2017-05-05T01:39:00Z","value":0},{"timestamp":"2017-05-05T01:40:00Z","value":0},{"timestamp":"2017-05-05T01:41:00Z","value":0},{"timestamp":"2017-05-05T01:42:00Z","value":0},{"timestamp":"2017-05-05T01:43:00Z","value":0},{"timestamp":"2017-05-05T01:44:00Z","value":0},{"timestamp":"2017-05-05T01:45:00Z","value":0},{"timestamp":"2017-05-05T01:46:00Z","value":0},{"timestamp":"2017-05-05T01:47:00Z","value":0},{"timestamp":"2017-05-05T01:48:00Z","value":0},{"timestamp":"2017-05-05T01:49:00Z","value":0},{"timestamp":"2017-05-05T01:50:00Z","value":0}],"latestTimestamp":"2017-05-05T01:50:00Z"},{"metrics":[],"latestTimestamp":"0001-01-01T00:00:00Z"}]}




# 部署 EFK 插件

cd /root/kubernetes/cluster/addons/fluentd-elasticsearch
ls *.yaml


# 在配置文件里直接全路径也可以用

touch es-rbac.yaml
touch fluentd-es-rbac.yaml

#   配置 es-controller.yaml

vi es-controller.yaml
# serviceAccountName: elasticsearch
# 及镜像
registry.cn-hangzhou.aliyuncs.com/google_images/elasticsearch:v2.4.1-2

#   配置 es-service.yaml

#   配置 fluentd-es-ds.yaml

vi fluentd-es-ds.yaml
# serviceAccountName: fluentd
# 及镜像
registry.cn-hangzhou.aliyuncs.com/google_images/fluentd-elasticsearch:1.22

#   配置 kibana-controller.yaml
vi kibana-controller.yaml
registry.cn-hangzhou.aliyuncs.com/google_images/kibana:v4.6.1-1

#   给 Node 设置标签
# DaemonSet fluentd-es-v1.22 只会调度到设置了标签 beta.kubernetes.io/fluentd-ds-ready=true 的 Node，
# 需要在期望运行 fluentd 的 Node 上设置该标签  
# nodeSelector:

kubectl get nodes
kubectl label nodes 10.64.3.7 beta.kubernetes.io/fluentd-ds-ready=true

#   执行定义文件
pwd
ls *.yaml
kubectl create -f .

# replicationcontroller "elasticsearch-logging-v1" created
# serviceaccount "elasticsearch" created

# clusterrolebinding "elasticsearch" created
# service "elasticsearch-logging" created

# daemonset "fluentd-es-v1.22" created
# serviceaccount "fluentd" created

# clusterrolebinding "fluentd" created
# deployment "kibana-logging" created
# service "kibana-logging" created


#   检查执行结果
kubectl get deployment -n kube-system|grep kibana
kubectl get pods -n kube-system|grep -E 'elasticsearch|fluentd|kibana'
kubectl get service  -n kube-system|grep -E 'elasticsearch|kibana'

kubectl logs kibana-logging-324921636-v8hk7 -n kube-system -f
#   访问 kibana
kubectl cluster-info
http://10.64.3.7:8080/api/v1/proxy/namespaces/kube-system/services/kibana-logging

kubectl proxy --address='10.64.3.7' --port=8086 --accept-hosts='^*$'
http://10.64.3.7:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging

# 查看信息
curl http://elasticsearch-logging:9200
curl http://elasticsearch-logging:9200/?pretty
curl -XGET http://elasticsearch-logging:9200/?pretty

# 集群概要
curl -XGET http://elasticsearch-logging:9200/_cluster/stats?pretty

curl -XGET http://elasticsearch-logging:9200/_cluster/health?pretty
curl -XGET http://elasticsearch-logging:9200/_cluster/settings?pretty



# 暂时故障排除, node无法访问, 关闭后,可以看到 7号机的日志
# 重启后故障消除,无法继续排查, 并未解决, 2个pod全都 7 了

# 哪怕到了 k8s 年代,也是重启解百病啊...

# 部署 Docker Registry

 # registry v2
# 部署一个 TLS 加密、HTTP Basic 认证、用 ceph rgw 做后端存储
# 暂时先不要布吧,感觉用的不多

# ceph rgw: 10.64.3.9
# docker registry: 10.64.3.7

#   部署 ceph RGW 节点
ceph-deploy rgw create 10.64.3.9

#   创建测试账号 demo
radosgw-admin user create --uid=demo --display-name="ceph rgw demo user"

#   创建 demo 账号的子账号 swift
# 当前 registry 只支持使用 swift 协议访问 ceph rgw 存储，暂时不支持 s3 协议
radosgw-admin subuser create --uid demo --subuser=demo:swift --access=full --secret=secretkey --key-type=swift


#   创建 demo:swift 子账号的 sercret key
radosgw-admin key create --subuser=demo:swift --key-type=swift --gen-secret

    # "swift_keys": [
    #     {
    #         "user": "demo:swift",
    #         "secret_key": "aCgVTx3Gfz1dBiFS4NfjIRmvT0sgpHDP6aa0Yfrh"
    #     }
    # ],




#   创建 docker registry
mdir -p registry/{auth,certs}

registry-csr.json

cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/etc/kubernetes/ssl/ca-config.json \
  -profile=kubernetes registry-csr.json | cfssljson -bare registry
cp registry.pem registry-key.pem registry/certs

# 证书应该是在3.7上执行的

# 创建 HTTP Baisc 认证文件
docker run --entrypoint htpasswd registry:2 -Bbn foo foo123  > auth/htpasswd
cat auth/htpasswd

# 配置参数
export RGW_AUTH_URL="http://10.64.3.9:7480/auth/v1"
export RGW_USER="demo:swift"
export RGW_SECRET_KEY="aCgVTx3Gfz1dBiFS4NfjIRmvT0sgpHDP6aa0Yfrh"

config.yml

# storage.swift 指定后端使用 swfit 接口协议的存储，这里配置的是 ceph rgw 存储参数；
# auth.htpasswd 指定了 HTTP Basic 认证的 token 文件路径；
# http.tls 指定了 registry http 服务器的证书和秘钥文件路径；



# 创建 docker registry

docker run -d -p 8000:8000 \
    -v $(pwd)/registry/auth/:/auth \
    -v $(pwd)/registry/certs:/certs \
    -v $(pwd)/config.yml:/etc/docker/registry/config.yml \
    --name registry registry:2

# 执行该 docker run 命令的机器 IP 为 10.64.3.7

# 这段写的不太好, ceph不熟,不知道在哪操作

#   向 registry push image
# 将签署 registry 证书的 CA 证书拷贝到 /etc/docker/certs.d/10.64.3.7:8000 目录下

sudo mkdir -p /etc/docker/certs.d/10.64.3.7:8000
sudo cp /etc/kubernetes/ssl/ca.pem /etc/docker/certs.d/10.64.3.7:8000/ca.crt

docker login 10.64.3.7:8000

cat ~/.docker/config.json

# 将本地的 image 打上私有 registry 的 tag
docker tag docker.io/kubernetes/pause 10.64.3.7:8000/zhangjun3/pause
docker images |grep pause

# 将 image push 到私有 registry
docker push 10.64.3.7:8000/zhangjun3/pause


# 查看 ceph 上是否已经有 push 的 pause 容器文件
rados lspools
rados --pool default.rgw.buckets.data ls|grep pause

#   私有 registry 的运维操作
#     查询私有镜像中的 images
curl  --user zhangjun3:xxx --cacert /etc/docker/certs.d/10.64.3.7\:8000/ca.crt https://10.64.3.7:8000/v2/_catalog

#     查询某个镜像的 tags 列表
curl  --user zhangjun3:xxx --cacert /etc/docker/certs.d/10.64.3.7\:8000/ca.crt https://10.64.3.7:8000/v2/zhangjun3/busybox/tags/list

#     获取 image 或 layer 的 digest
# 向 v2/<repoName>/manifests/<tagName> 发 GET 请求，从响应的头部 Docker-Content-Digest 获取 image digest，从响应的 body 的 fsLayers.blobSum 中获取 layDigests;
# 注意，必须包含请求头：Accept: application/vnd.docker.distribution.manifest.v2+json

curl -v -H "Accept: application/vnd.docker.distribution.manifest.v2+json" --user zhangjun3:xxx --cacert /etc/docker/certs.d/10.64.3.7\:8000/ca.crt https://10.64.3.7:8000/v2/zhangjun3/busybox/manifests/latest

#     删除 image
curl -X DELETE  --user zhangjun3:xxx --cacert /etc/docker/certs.d/10.64.3.7\:8000/ca.crt https://10.64.3.7:8000/v2/zhangjun3/busybox/manifests/sha256:68effe31a4ae8312e47f54bec52d1fc925908009ce7e6f734e1b54a4169081c5

#     删除 layer
# 向 /v2/<name>/blobs/<digest>发送 DELETE 请求，其中 digest 是上一步返回的 fsLayers.blobSum 字段内容：

curl -X DELETE  --user zhangjun3:xxx --cacert /etc/docker/certs.d/10.64.3.7\:8000/ca.crt https://10.64.3.7:8000/v2/zhangjun3/busybox/blobs/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4
curl -X DELETE  --cacert /etc/docker/certs.d/10.64.3.7\:8000/ca.crt https://10.64.3.7:8000/v2/zhangjun3/busybox/blobs/sha256:04176c8b224aa0eb9942af765f66dae866f436e75acef028fe44b8a98e045515
# 熟悉一下,即可,不是主流


# 部署 Harbor 私有仓库
#   使用的变量
export NODE_IP=10.64.3.7

#   下载文件
wget https://github.com/docker/compose/releases/download/1.12.0/docker-compose-Linux-x86_64
mv ~/docker-compose-Linux-x86_64 /root/local/bin/docker-compose
chmod a+x  /root/local/bin/docker-compose
export PATH=/root/local/bin:$PATH

# wget  --continue https://github.com/vmware/harbor/releases/download/v1.1.0/harbor-offline-installer-v1.1.0.tgz
wget  --continue http://harbor.orientsoft.cn/harbor-1.1.1/harbor-offline-installer-v1.1.1.tgz
tar -xzvf harbor-offline-installer-v1.1.1.tgz
cd harbor

#   导入 docker images
# 导入离线安装包中 harbor 相关的 docker images：

docker load -i harbor.v1.1.1.tar.gz

#   创建 harbor nginx 服务器使用的 TLS 证书
harbor-csr.json

 cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
 -ca-key=/etc/kubernetes/ssl/ca-key.pem \
 -config=/etc/kubernetes/ssl/ca-config.json \
 -profile=kubernetes harbor-csr.json | cfssljson -bare harbor
 
 ls harbor*
 # harbor.csr  harbor-csr.json  harbor-key.pem harbor.pem

 sudo mkdir -p /etc/harbor/ssl
 sudo mv harbor*.pem /etc/harbor/ssl
 rm harbor.csr  harbor-csr.json

#   修改 harbor.cfg 文件

vi harbor.cfg
# hostname, ui_url_protocol,证书,原有值不能保留

#   加载和启动 harbor 镜像
./install.sh
# 应该是首次使用

#   访问管理界面
https://10.64.3.7

# 用账号 admin 和 harbor.cfg 配置文件中的默认密码 Harbor12345 登陆系统

#   harbor 运行时产生的文件、目录
ls /var/log/harbor/2019-06-25/
ls /data/

#   docker 客户端登陆
# 将签署 harbor 证书的 CA 证书拷贝到 /etc/docker/certs.d/10.64.3.7 目录下
sudo mkdir -p /etc/docker/certs.d/10.64.3.7
sudo cp /etc/kubernetes/ssl/ca.pem /etc/docker/certs.d/10.64.3.7/ca.crt

docker login 10.64.3.7
# 认证信息自动保存到 ~/.docker/config.json 文件

#   其它操作

# 停止 harbor
docker-compose down -v

# 修改配置
vi harbor.cfg

# 更修改的配置更新到 docker-compose.yml 文件
./prepare

# 启动 harbor
docker ps -a |grep ui
docker rm xx
docker-compose up -d

# 把上一章的练习拿过来用一下

docker pull registry.docker-cn.com/library/ubuntu:16.04

vi /etc/docker/daemon.json增加如下内容
{
  "registry-mirrors": ["https://registry.docker-cn.com"]
}

# 保存配置后，重启Docker进行生效



# 检查方法
kubectl logs kibana-logging-2887055332-9m8hb -n kube-system -f  
# 查看日志


# Pod一直处于ContainerCreating状态
    # 在Kubernetes环境网络可达的镜像私服上放一个pause-amd64的镜像，一般都是3.0版本
    # 修改Kubernetes各节点的/etc/kubernetes/kubelet文件，追加
    KUBELET_ARGS="--pod_infra_container_image=192.168.10.12:5000/google_containers/pause-amd64.3.0"
    # 重启master节点的kube-apiserver kube-controller-manager kube-scheduler kube-proxy服务

# 公网上的容器镜像放到镜像私服
    # 外网docker环境中使用docker pull 命令拉取镜像到本地
    # 使用docker save -o 命令将镜像保存为tar格式的文件
    # 将tar文件拷贝至内网docker环境，然后使用docker load < XX.tar命令将镜像load到内网docker环境
    # 将导入后的镜像，使用docker tag <私服地址>/:命令，为镜像打标签
    # 使用docker push<私服地址>/:将镜像push到私服上

# 需要补充命令实例




